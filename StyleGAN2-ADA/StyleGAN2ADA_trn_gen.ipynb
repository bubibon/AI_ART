{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9olWAtrQi5YY"
   },
   "source": [
    "# StyleGan2-ADA\n",
    "\n",
    "official paper: https://arxiv.org/abs/2006.06676\n",
    "\n",
    "official repo: https://github.com/NVlabs/stylegan2-ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upA8YlB4SUtP",
    "outputId": "deab482a-ebc6-427a-efd6-679260dce8f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo5QAWzGbFjt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVd63kaLt7_3",
    "outputId": "e10e73f7-b3b2-4ac5-9e27-0d00d5653a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "#In order to train the model this version of numpy must be installed\n",
    "%pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7t-lSC5fFvs",
    "outputId": "d8582eca-967b-40d3-982d-7ed9be2a76f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Thu Feb 24 12:58:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#In order to train the model this version of TensorFlow must be installed\n",
    "%tensorflow_version 1.x\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3PrK4P1aQix"
   },
   "source": [
    "## Preparing  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_3sjr0paPq2",
    "outputId": "c5b765d0-6f64-49b0-e2e6-a881ebadc943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:2800: DecompressionBombWarning: Image size (96709923 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! :) A total of:  3562 images has been resized to 1024 x 1024\n"
     ]
    }
   ],
   "source": [
    "# Defining an image size and image channel\n",
    "# We are going to resize all our images to 1024x1024 size and since our images are colored images\n",
    "# We are setting our image channels to 3 (RGB)\n",
    "\n",
    "IMAGE_SIZE = 1024\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_DIR = '/content/drive/MyDrive/tesi2/images'\n",
    "\n",
    "# Defining image dir path. Change this if you have different directory\n",
    "images_path = IMAGE_DIR\n",
    "output_dataset_path = '/content/drive/MyDrive/tesi2/resized_jap_1024'\n",
    "\n",
    "# Iterating over the images inside the directories and resizing them using\n",
    "# Pillow's resize method.\n",
    "print('resizing...')\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for artist in os.listdir(images_path):\n",
    "  \n",
    "  artist_path = os.path.join(images_path,artist)\n",
    "  \n",
    "  for file in os.listdir(artist_path):\n",
    "  \n",
    "    path = os.path.join(artist_path,file)\n",
    "  \n",
    "    im = Image.open(path)\n",
    "  \n",
    "    image = im.convert('RGB')\n",
    "  \n",
    "    image = image.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "\n",
    "    filename = os.path.join(output_dataset_path, f\"resized-{cnt}.png\")\n",
    "  \n",
    "    image.save(filename)\n",
    "\n",
    "    cnt +=1\n",
    "\n",
    "print(\"All done! :) A total of: \", len(os.listdir(output_dataset_path)), \"images has been resized to\", IMAGE_SIZE, \"x\",IMAGE_SIZE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54RNfosM0mVs"
   },
   "source": [
    "**NB**: at this point some other images were manually deleted as they weren't fit for the purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sum5CSsMmwAf"
   },
   "source": [
    "### Images --> TFR\n",
    "\n",
    "StyleGAN2-ADA takes as inpunt images in TFR format, for this reason the 1024x1024 images have to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcQkqxTGnZYC"
   },
   "outputs": [],
   "source": [
    "#Creating a repo where we will store the tfr\n",
    "%mkdir 'path/to/tfr_repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4-Gbhszm1i5",
    "outputId": "2dfc21b3-5f34-4ee1-e580-71bb25891cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"/content/drive/MyDrive/HED/out\"\n",
      "Creating dataset \"/content/drive/MyDrive/StyleGAN2-ADA/tfr_sketch\"\n",
      "/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
      "Added 5000 images.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'path/to/1024x1024images'\n",
    "tfr_path = 'path/to/tfr_repo'\n",
    "\n",
    "!python \"stylegan2_repo_path/dataset_tool.py\" create_from_images {tfr_path} {dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPY_14f8oCtG"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NYxzQrKSJ-v",
    "outputId": "fab14083-f918-4393-d2f4-823232602f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Sun Feb 27 16:44:08 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Be sure to have tf 1.x version installed!!\n",
    "%tensorflow_version 1.x\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2dG-lsD-P89",
    "outputId": "3f23e38d-1581-46c7-b3d2-a89fac5000cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
      "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
      "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
      "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
      "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
      "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
      "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
      "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
      "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
      "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
      "\n",
      "Train a GAN using the techniques described in the paper\n",
      "\"Training Generative Adversarial Networks with Limited Data\".\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "general options:\n",
      "  --outdir DIR          Where to save the results (required)\n",
      "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
      "  --snap INT            Snapshot interval (default: 50 ticks)\n",
      "  --seed INT            Random seed (default: 1000)\n",
      "  -n, --dry-run         Print training options and exit\n",
      "\n",
      "training dataset:\n",
      "  --data PATH           Training dataset path (required)\n",
      "  --res INT             Dataset resolution (default: highest available)\n",
      "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
      "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
      "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
      "                        create_from_images_raw (default: False)\n",
      "\n",
      "metrics:\n",
      "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
      "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
      "\n",
      "base config:\n",
      "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
      "                        Base config (default: auto)\n",
      "  --lrate FLOAT         Override learning rate\n",
      "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
      "                        for discriminator) (default: false)\n",
      "  --gamma FLOAT         Override R1 gamma\n",
      "  --nkimg INT           Override starting count\n",
      "  --kimg INT            Override training duration\n",
      "  --topk FLOAT          utilize top-k training\n",
      "\n",
      "discriminator augmentation:\n",
      "  --aug {noaug,ada,fixed,adarv}\n",
      "                        Augmentation mode (default: ada)\n",
      "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
      "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
      "  --initstrength INITSTRENGTH\n",
      "                        Override ADA strength at start\n",
      "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
      "                        Augmentation pipeline (default: bgc)\n",
      "\n",
      "comparison methods:\n",
      "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
      "                        Comparison method (default: nocmethod)\n",
      "  --dcap FLOAT          Multiplier for discriminator capacity\n",
      "\n",
      "transfer learning:\n",
      "  --resume RESUME       Resume from network pickle (default: noresume)\n",
      "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
      "\n",
      "examples:\n",
      "\n",
      "  # Train custom dataset using 1 GPU.\n",
      "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
      "\n",
      "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
      "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
      "      --cfg=cifar\n",
      "\n",
      "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
      "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
      "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
      "\n",
      "  # Reproduce original StyleGAN2 config F.\n",
      "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
      "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
      "\n",
      "available base configs (--cfg):\n",
      "  auto           Automatically select reasonable defaults based on resolution\n",
      "                 and GPU count. Good starting point for new datasets.\n",
      "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
      "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
      "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
      "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
      "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
      "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
      "\n",
      "transfer learning source networks (--resume):\n",
      "  ffhq256        FFHQ trained at 256x256 resolution.\n",
      "  ffhq512        FFHQ trained at 512x512 resolution.\n",
      "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
      "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
      "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
      "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
      "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
      "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
      "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
      "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
      "  metfaces512    MetFaces trained at 512x512 resolution.\n",
      "  <path or URL>  Custom network pickle.\n"
     ]
    }
   ],
   "source": [
    "!python \"stylegan2_repo_path/train.py\" --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXJrtKkTICbG",
    "outputId": "488b6315-ee23-43ca-c529-87d223db1177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 645, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 637, in main\n",
      "    run_training(**vars(args))\n",
      "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/train.py\", line 522, in run_training\n",
      "    training_loop.training_loop(**training_options)\n",
      "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/training/training_loop.py\", line 255, in training_loop\n",
      "    tflib.run([D_train_op, Gs_update_op, Gs_epochs_op], {Gs_beta_in: Gs_beta, Gs_epochs: epochs})\n",
      "  File \"/content/drive/MyDrive/StyleGAN2-ADA/stylegan2-ada/dnnlib/tflib/tfutil.py\", line 33, in run\n",
      "    return tf.get_default_session().run(*args, **kwargs)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python \"stylegan2_repo_path/train.py\" --outdir=\"path/to/output_repo\" --data=\"'path/to/tfr_repo'\" \\\n",
    "    --snap=2 --augpipe='bgc'\\\n",
    "    --mirror=True --metrics=None --mirrory=False\n",
    "\n",
    "\"\"\"\n",
    "TRANSFER LEARNING: use the --resume='' option with the path to a pretrained net saved in a pkl file\n",
    "\n",
    "ex:\n",
    "\n",
    "!python \"stylegan2_repo_path/train.py\" --outdir=\"path/to/output_repo\" --data=\"'path/to/tfr_repo'\" \\\n",
    "    --snap=2 --augpipe='bgc' --resume='path/to/pretrained_models_repo/pretrained_net.pkl'\\\n",
    "    --mirror=True --metrics=None --mirrory=False --freezed=13\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6ZvoMI_UVoP"
   },
   "source": [
    "# Generate images\n",
    "NB: If you generate images and then want to train again the model, be sure to install numpy 1.19 again as the next install will update it to 1.21.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JHZOPAHRGPQ",
    "outputId": "2872ab7f-71d8-4029-8d3a-df1f91bbb83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opensimplex\n",
      "  Downloading opensimplex-0.4.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from opensimplex) (1.21.5)\n",
      "Installing collected packages: opensimplex\n",
      "Successfully installed opensimplex-0.4.2\n"
     ]
    }
   ],
   "source": [
    "%pip install opensimplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jU3MDJ3pF8JQ",
    "outputId": "7022f9e7-ae06-488e-f240-c64f80697bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: generate.py generate-images [-h] --network NETWORK_PKL --seeds SEEDS\n",
      "                                   [--trunc TRUNCATION_PSI]\n",
      "                                   [--class CLASS_IDX] [--create-grid]\n",
      "                                   [--outdir DIR] [--save_vector] [--fixnoise]\n",
      "                                   [--jpg_quality JPG_QUALITY]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --network NETWORK_PKL\n",
      "                        Network pickle filename\n",
      "  --seeds SEEDS         List of random seeds\n",
      "  --trunc TRUNCATION_PSI\n",
      "                        Truncation psi (default: 0.5)\n",
      "  --class CLASS_IDX     Class label (default: unconditional)\n",
      "  --create-grid         Add flag to save the generated images in a grid\n",
      "  --outdir DIR          Root directory for run results (default: out)\n",
      "  --save_vector         also save vector in .npy format\n",
      "  --fixnoise            generate images using fixed noise (more accurate for\n",
      "                        interpolations)\n",
      "  --jpg_quality JPG_QUALITY\n",
      "                        Quality compression for JPG exports (1 to 95), keep\n",
      "                        default value to export as PNG\n"
     ]
    }
   ],
   "source": [
    "!python \"stylegan2_repo_path/generate.py\" generate-images --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFFLezqD2knS",
    "outputId": "cd7d37ea-a25a-4c0b-b136-cbfc3c719c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"/content/drive/MyDrive/StyleGAN2-ADA/training_imp_scratch/00009-tfr_imp-mirror-auto1-bgc-resumecustom/network-snapshot-000040.pkl\"...\n",
      "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
      "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
      "Generating image for seed 0 (0/2) ...\n",
      "Generating image for seed 1 (1/2) ...\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "seed_init = random.randint(1)\n",
    "nbr_images = 2\n",
    "\n",
    "#insert here the path to the pretrained model you want to generate images with\n",
    "generation_from = 'path/to/pretrained_models_repo/pretrained_net.pkl'\n",
    "\n",
    "!python \"stylegan2_repo_path/generate.py\" generate-images \\\n",
    "    --outdir=\"path/to/output_repo/generated_repo\" --trunc=0.5 \\\n",
    "    --seeds={seed_init}-{seed_init+nbr_images-1} \\\n",
    "    --network={generation_from}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQiXIBBU9ro"
   },
   "source": [
    "## Latent space walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyNp4yxvQ0Q9"
   },
   "outputs": [],
   "source": [
    "%pip install opensimplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4qcW3J7U_if",
    "outputId": "c8e503ce-a307-4da6-cf40-afd2a98d1152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5419,7241,4163,4072,3035,1258,6685,9753,4483,5436\n",
      "Base seeds: [5419, 7241, 4163, 4072, 3035, 1258, 6685, 9753, 4483, 5436]\n",
      "Loading networks from \"/content/drive/MyDrive/StyleGAN2-ADA/training_jap/00000-tfr_jap-mirror-auto1-bgc-resumeffhq1024/network-snapshot-000088.pkl\"...\n",
      "Setting up TensorFlow plugin \"fused_bias_act.cu\": Loading... Done.\n",
      "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Loading... Done.\n",
      "slerp\n",
      "Generating image for step 0/207 ...\n",
      "Generating image for step 1/207 ...\n",
      "Generating image for step 2/207 ...\n",
      "Generating image for step 3/207 ...\n",
      "Generating image for step 4/207 ...\n",
      "Generating image for step 5/207 ...\n",
      "Generating image for step 6/207 ...\n",
      "Generating image for step 7/207 ...\n",
      "Generating image for step 8/207 ...\n",
      "Generating image for step 9/207 ...\n",
      "Generating image for step 10/207 ...\n",
      "Generating image for step 11/207 ...\n",
      "Generating image for step 12/207 ...\n",
      "Generating image for step 13/207 ...\n",
      "Generating image for step 14/207 ...\n",
      "Generating image for step 15/207 ...\n",
      "Generating image for step 16/207 ...\n",
      "Generating image for step 17/207 ...\n",
      "Generating image for step 18/207 ...\n",
      "Generating image for step 19/207 ...\n",
      "Generating image for step 20/207 ...\n",
      "Generating image for step 21/207 ...\n",
      "Generating image for step 22/207 ...\n",
      "Generating image for step 23/207 ...\n",
      "Generating image for step 24/207 ...\n",
      "Generating image for step 25/207 ...\n",
      "Generating image for step 26/207 ...\n",
      "Generating image for step 27/207 ...\n",
      "Generating image for step 28/207 ...\n",
      "Generating image for step 29/207 ...\n",
      "Generating image for step 30/207 ...\n",
      "Generating image for step 31/207 ...\n",
      "Generating image for step 32/207 ...\n",
      "Generating image for step 33/207 ...\n",
      "Generating image for step 34/207 ...\n",
      "Generating image for step 35/207 ...\n",
      "Generating image for step 36/207 ...\n",
      "Generating image for step 37/207 ...\n",
      "Generating image for step 38/207 ...\n",
      "Generating image for step 39/207 ...\n",
      "Generating image for step 40/207 ...\n",
      "Generating image for step 41/207 ...\n",
      "Generating image for step 42/207 ...\n",
      "Generating image for step 43/207 ...\n",
      "Generating image for step 44/207 ...\n",
      "Generating image for step 45/207 ...\n",
      "Generating image for step 46/207 ...\n",
      "Generating image for step 47/207 ...\n",
      "Generating image for step 48/207 ...\n",
      "Generating image for step 49/207 ...\n",
      "Generating image for step 50/207 ...\n",
      "Generating image for step 51/207 ...\n",
      "Generating image for step 52/207 ...\n",
      "Generating image for step 53/207 ...\n",
      "Generating image for step 54/207 ...\n",
      "Generating image for step 55/207 ...\n",
      "Generating image for step 56/207 ...\n",
      "Generating image for step 57/207 ...\n",
      "Generating image for step 58/207 ...\n",
      "Generating image for step 59/207 ...\n",
      "Generating image for step 60/207 ...\n",
      "Generating image for step 61/207 ...\n",
      "Generating image for step 62/207 ...\n",
      "Generating image for step 63/207 ...\n",
      "Generating image for step 64/207 ...\n",
      "Generating image for step 65/207 ...\n",
      "Generating image for step 66/207 ...\n",
      "Generating image for step 67/207 ...\n",
      "Generating image for step 68/207 ...\n",
      "Generating image for step 69/207 ...\n",
      "Generating image for step 70/207 ...\n",
      "Generating image for step 71/207 ...\n",
      "Generating image for step 72/207 ...\n",
      "Generating image for step 73/207 ...\n",
      "Generating image for step 74/207 ...\n",
      "Generating image for step 75/207 ...\n",
      "Generating image for step 76/207 ...\n",
      "Generating image for step 77/207 ...\n",
      "Generating image for step 78/207 ...\n",
      "Generating image for step 79/207 ...\n",
      "Generating image for step 80/207 ...\n",
      "Generating image for step 81/207 ...\n",
      "Generating image for step 82/207 ...\n",
      "Generating image for step 83/207 ...\n",
      "Generating image for step 84/207 ...\n",
      "Generating image for step 85/207 ...\n",
      "Generating image for step 86/207 ...\n",
      "Generating image for step 87/207 ...\n",
      "Generating image for step 88/207 ...\n",
      "Generating image for step 89/207 ...\n",
      "Generating image for step 90/207 ...\n",
      "Generating image for step 91/207 ...\n",
      "Generating image for step 92/207 ...\n",
      "Generating image for step 93/207 ...\n",
      "Generating image for step 94/207 ...\n",
      "Generating image for step 95/207 ...\n",
      "Generating image for step 96/207 ...\n",
      "Generating image for step 97/207 ...\n",
      "Generating image for step 98/207 ...\n",
      "Generating image for step 99/207 ...\n",
      "Generating image for step 100/207 ...\n",
      "Generating image for step 101/207 ...\n",
      "Generating image for step 102/207 ...\n",
      "Generating image for step 103/207 ...\n",
      "Generating image for step 104/207 ...\n",
      "Generating image for step 105/207 ...\n",
      "Generating image for step 106/207 ...\n",
      "Generating image for step 107/207 ...\n",
      "Generating image for step 108/207 ...\n",
      "Generating image for step 109/207 ...\n",
      "Generating image for step 110/207 ...\n",
      "Generating image for step 111/207 ...\n",
      "Generating image for step 112/207 ...\n",
      "Generating image for step 113/207 ...\n",
      "Generating image for step 114/207 ...\n",
      "Generating image for step 115/207 ...\n",
      "Generating image for step 116/207 ...\n",
      "Generating image for step 117/207 ...\n",
      "Generating image for step 118/207 ...\n",
      "Generating image for step 119/207 ...\n",
      "Generating image for step 120/207 ...\n",
      "Generating image for step 121/207 ...\n",
      "Generating image for step 122/207 ...\n",
      "Generating image for step 123/207 ...\n",
      "Generating image for step 124/207 ...\n",
      "Generating image for step 125/207 ...\n",
      "Generating image for step 126/207 ...\n",
      "Generating image for step 127/207 ...\n",
      "Generating image for step 128/207 ...\n",
      "Generating image for step 129/207 ...\n",
      "Generating image for step 130/207 ...\n",
      "Generating image for step 131/207 ...\n",
      "Generating image for step 132/207 ...\n",
      "Generating image for step 133/207 ...\n",
      "Generating image for step 134/207 ...\n",
      "Generating image for step 135/207 ...\n",
      "Generating image for step 136/207 ...\n",
      "Generating image for step 137/207 ...\n",
      "Generating image for step 138/207 ...\n",
      "Generating image for step 139/207 ...\n",
      "Generating image for step 140/207 ...\n",
      "Generating image for step 141/207 ...\n",
      "Generating image for step 142/207 ...\n",
      "Generating image for step 143/207 ...\n",
      "Generating image for step 144/207 ...\n",
      "Generating image for step 145/207 ...\n",
      "Generating image for step 146/207 ...\n",
      "Generating image for step 147/207 ...\n",
      "Generating image for step 148/207 ...\n",
      "Generating image for step 149/207 ...\n",
      "Generating image for step 150/207 ...\n",
      "Generating image for step 151/207 ...\n",
      "Generating image for step 152/207 ...\n",
      "Generating image for step 153/207 ...\n",
      "Generating image for step 154/207 ...\n",
      "Generating image for step 155/207 ...\n",
      "Generating image for step 156/207 ...\n",
      "Generating image for step 157/207 ...\n",
      "Generating image for step 158/207 ...\n",
      "Generating image for step 159/207 ...\n",
      "Generating image for step 160/207 ...\n",
      "Generating image for step 161/207 ...\n",
      "Generating image for step 162/207 ...\n",
      "Generating image for step 163/207 ...\n",
      "Generating image for step 164/207 ...\n",
      "Generating image for step 165/207 ...\n",
      "Generating image for step 166/207 ...\n",
      "Generating image for step 167/207 ...\n",
      "Generating image for step 168/207 ...\n",
      "Generating image for step 169/207 ...\n",
      "Generating image for step 170/207 ...\n",
      "Generating image for step 171/207 ...\n",
      "Generating image for step 172/207 ...\n",
      "Generating image for step 173/207 ...\n",
      "Generating image for step 174/207 ...\n",
      "Generating image for step 175/207 ...\n",
      "Generating image for step 176/207 ...\n",
      "Generating image for step 177/207 ...\n",
      "Generating image for step 178/207 ...\n",
      "Generating image for step 179/207 ...\n",
      "Generating image for step 180/207 ...\n",
      "Generating image for step 181/207 ...\n",
      "Generating image for step 182/207 ...\n",
      "Generating image for step 183/207 ...\n",
      "Generating image for step 184/207 ...\n",
      "Generating image for step 185/207 ...\n",
      "Generating image for step 186/207 ...\n",
      "Generating image for step 187/207 ...\n",
      "Generating image for step 188/207 ...\n",
      "Generating image for step 189/207 ...\n",
      "Generating image for step 190/207 ...\n",
      "Generating image for step 191/207 ...\n",
      "Generating image for step 192/207 ...\n",
      "Generating image for step 193/207 ...\n",
      "Generating image for step 194/207 ...\n",
      "Generating image for step 195/207 ...\n",
      "Generating image for step 196/207 ...\n",
      "Generating image for step 197/207 ...\n",
      "Generating image for step 198/207 ...\n",
      "Generating image for step 199/207 ...\n",
      "Generating image for step 200/207 ...\n",
      "Generating image for step 201/207 ...\n",
      "Generating image for step 202/207 ...\n",
      "Generating image for step 203/207 ...\n",
      "Generating image for step 204/207 ...\n",
      "Generating image for step 205/207 ...\n",
      "Generating image for step 206/207 ...\n",
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, image2, from '/content/drive/MyDrive/StyleGAN2-ADA/out/latent_walk/frames/frame%05d.png':\n",
      "  Duration: 00:00:08.28, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 1024x1024, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mprofile High, level 3.2\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/content/drive/MyDrive/StyleGAN2-ADA/out/latent_walk/walk-z-sphere-seed0-24fps.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x1024, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  207 fps= 20 q=-1.0 Lsize=    7227kB time=00:00:08.50 bitrate=6965.0kbits/s speed=0.829x    \n",
      "video:7224kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.042611%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mframe I:1     Avg QP:23.45  size:163461\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mframe P:90    Avg QP:25.42  size: 60263\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mframe B:116   Avg QP:28.93  size: 15599\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mconsecutive B-frames: 22.2%  7.7%  4.3% 65.7%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mmb I  I16..4:  7.9% 57.2% 35.0%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mmb P  I16..4:  1.4%  7.4%  4.4%  P16..4: 41.4% 20.8% 16.2%  0.0%  0.0%    skip: 8.4%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mmb B  I16..4:  0.1%  0.3%  0.4%  B16..8: 21.5%  7.7%  4.3%  direct:10.8%  skip:54.8%  L0:29.5% L1:37.5% BI:33.0%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0m8x8 transform intra:54.8% inter:53.9%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mcoded y,uvDC,uvAC intra: 88.4% 73.5% 43.4% inter: 35.4% 30.6% 2.5%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mi16 v,h,dc,p: 41% 48%  4%  7%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 18% 21%  6%  7%  6%  9%  7% 12%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 19% 19% 14%  7%  8%  8%  9%  6%  9%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mi8c dc,h,v,p: 43% 29% 21%  8%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mWeighted P-Frames: Y:63.3% UV:51.1%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mref P L0: 60.6% 25.1% 11.4%  2.1%  0.9%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mref B L0: 95.2%  2.8%  2.0%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mref B L1: 98.9%  1.1%\n",
      "\u001b[1;36m[libx264 @ 0x55a8b5cefe00] \u001b[0mkb/s:6860.56\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "walk_types = ['line', 'sphere', 'noiseloop', 'circularloop']\n",
    "latent_walk_path = 'path/to/output_repo/latent_walk'\n",
    "\n",
    "#insert here the path to the pretrained model you want to explore\n",
    "explored_network = 'path/to/pretrained_models_repo/pretrained_net.pkl'\n",
    "\n",
    "seeds = [random.randint(10000) for i in range(10)]\n",
    "print(','.join(map(str, seeds)))\n",
    "print(\"Base seeds:\", seeds)\n",
    "!python \"stylegan2_repo_path/generate.py\" generate-latent-walk --network=\"{explored_network}\" \\\n",
    "    --outdir=\"{latent_walk_path}\" --trunc=0.7 --walk-type=\"{walk_types[1]}\" \\\n",
    "    --seeds={','.join(map(str, seeds))} --frames {len(seeds)*20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ehxwOn1VjnS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "I3PrK4P1aQix"
   ],
   "machine_shape": "hm",
   "name": "Style_GAN_JAPPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
